# informationTheoryToolkit2
a tool for computing and analyzing entropy differences in both discrete and continuous random variables.

Project

• Compute the difference between the entropy of a discrete random variable given its p.m.f. vector (choose some p.m.f. vectors to test) and the
entropy of an estimated p.m.f. from a set of samples generated through
the same p.m.f. vector. You can choose freely one p.m.f. vector.

• Write a function called "differential entropy" which computes the differential entropy of a generic continuous random variable given its p.d.f.

• Compute the difference between the differential entropy of a Gaussian
continuous random variable given its p.d.f. vector and the differential
entropy of an estimated p.d.f. from a set of samples generated through
the same p.d.f. vector. You can choose freely the mean and variance.
